{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "kashifshariff12@gmail.com_26.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wVIx_KIigxPV",
        "outputId": "036f7c14-77a4-45e7-b163-601235cbaa4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        }
      },
      "source": [
        "# import keras\n",
        "# from keras.datasets import cifar10\n",
        "# from keras.models import Model, Sequential\n",
        "# from keras.layers import Dense, Dropout, Flatten, Input, AveragePooling2D, merge, Activation\n",
        "# from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
        "# from keras.layers import Concatenate\n",
        "# from keras.optimizers import Adam\n",
        "from tensorflow.keras import models, layers\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import BatchNormalization, Activation, Flatten\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UNHw6luQg3gc",
        "outputId": "35d1f193-abcf-4cfa-a1b4-b1958fa3b988",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# this part will prevent tensorflow to allocate all the avaliable GPU Memory\n",
        "# backend\n",
        "import tensorflow as tf\n",
        "# from tensorflow import keras\n",
        "\n",
        "from keras import backend as k\n",
        "\n",
        "# Don't pre-allocate memory; allocate as-needed\n",
        "# import tensorflow as tf\n",
        "#tf.config.gpu.set_per_process_memory_fraction(0.75)\n",
        "#tf.config.gpu.set_per_process_memory_growth(True)\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "\n",
        "# Create a session with the above options specified.\n",
        "k.tensorflow_backend.set_session(tf.Session(config=config))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dsO_yGxcg5D8",
        "colab": {}
      },
      "source": [
        "# Hyperparameters\n",
        "batch_size = 64\n",
        "num_classes = 10\n",
        "epochs = 300\n",
        "l = 8\n",
        "num_filter = 38\n",
        "compression = 0.94\n",
        "dropout_rate = 0.2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mB7o3zu1g6eT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "fce38077-d624-46bf-9ffb-9813853ad987"
      },
      "source": [
        "# Load CIFAR10 Data\n",
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "img_height, img_width, channel = X_train.shape[1],X_train.shape[2],X_train.shape[3]\n",
        "\n",
        "# convert to one hot encoing \n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes) "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 4s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ufWwJA_fFXa",
        "colab_type": "code",
        "outputId": "847df7a3-1369-492a-a06d-543b78f85b9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 32, 32, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdjbGzGEfFXm",
        "colab_type": "code",
        "outputId": "00d60633-3825-407f-bbac-829ad1c4e2bc",
        "colab": {}
      },
      "source": [
        "X_test.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 32, 32, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ee-sge5Kg7vr",
        "colab": {}
      },
      "source": [
        "# Dense Block\n",
        "def denseblock(input, num_filter = 12, dropout_rate = 0.2):\n",
        "    global compression\n",
        "    temp = input\n",
        "    for _ in range(l): \n",
        "        BatchNorm = layers.BatchNormalization()(temp)\n",
        "        relu = layers.Activation('relu')(BatchNorm)\n",
        "        Conv2D_3_3 = layers.Conv2D(int(num_filter*compression), (3,3), use_bias=False ,padding='same')(relu)\n",
        "        if dropout_rate>0:\n",
        "            Conv2D_3_3 = layers.Dropout(dropout_rate)(Conv2D_3_3)\n",
        "        concat = layers.Concatenate(axis=-1)([temp,Conv2D_3_3])\n",
        "        \n",
        "        temp = concat\n",
        "        \n",
        "    return temp\n",
        "\n",
        "## transition Blosck\n",
        "def transition(input, num_filter = 12, dropout_rate = 0.2):\n",
        "    global compression\n",
        "    BatchNorm = layers.BatchNormalization()(input)\n",
        "    relu = layers.Activation('relu')(BatchNorm)\n",
        "    Conv2D_BottleNeck = layers.Conv2D(int(num_filter*compression), (1,1), use_bias=False ,padding='same')(relu)\n",
        "    if dropout_rate>0:\n",
        "         Conv2D_BottleNeck = layers.Dropout(dropout_rate)(Conv2D_BottleNeck)\n",
        "    avg = layers.AveragePooling2D(pool_size=(2,2))(Conv2D_BottleNeck)\n",
        "    return avg\n",
        "\n",
        "#output layer\n",
        "def output_layer(input):\n",
        "    global compression\n",
        "    BatchNorm = layers.BatchNormalization()(input)\n",
        "    relu = layers.Activation('relu')(BatchNorm)\n",
        "    AvgPooling = layers.AveragePooling2D(pool_size=(2,2))(relu)\n",
        "    conv = layers.Conv2D(num_classes, (3,3), padding='same')(AvgPooling)\n",
        "    AvgPooling1 = layers.AveragePooling2D()(conv)\n",
        "    flat = layers.Flatten()(AvgPooling1)\n",
        "    output = layers.Activation('softmax')(flat)\n",
        "\n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "anPCpQWhhGb7",
        "colab": {}
      },
      "source": [
        "num_filter = 20\n",
        "dropout_rate = 0.15\n",
        "l = 12\n",
        "input = layers.Input(shape=(img_height, img_width, channel,))\n",
        "First_Conv2D = layers.Conv2D(num_filter, (3,3), use_bias=False ,padding='same')(input)\n",
        "\n",
        "First_Block = denseblock(First_Conv2D, num_filter, dropout_rate)\n",
        "First_Transition = transition(First_Block, num_filter, dropout_rate)\n",
        "\n",
        "Second_Block = denseblock(First_Transition, num_filter, dropout_rate)\n",
        "Second_Transition = transition(Second_Block, num_filter, dropout_rate)\n",
        "\n",
        "Third_Block = denseblock(Second_Transition, num_filter, dropout_rate)\n",
        "Third_Transition = transition(Third_Block, num_filter, dropout_rate)\n",
        "\n",
        "Last_Block = denseblock(Third_Transition,  num_filter, dropout_rate)\n",
        "output = output_layer(Last_Block)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2v5Qfq-fFYE",
        "colab_type": "code",
        "outputId": "6dcd2a12-2555-4aa4-fcf0-1bff466b883e",
        "colab": {}
      },
      "source": [
        "#https://arxiv.org/pdf/1608.06993.pdf\n",
        "from IPython.display import IFrame, YouTubeVideo\n",
        "YouTubeVideo(id='-W6y8xnd--U', width=600)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wCEABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2MBERISGBUYLxoaL2NCOEJjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY//AABEIAWgB4AMBIgACEQEDEQH/xAAbAAEAAgMBAQAAAAAAAAAAAAAAAQQCAwUGB//EAEsQAAIBAwAECAoGBwYGAwAAAAABAgMEEQUSITEGExQWQVFx0iIyVFVhgZGho9EVIzNSscEXNEJyk6LwJFNzgpLhQ0RiY4PxB2Sy/8QAGQEBAQEBAQEAAAAAAAAAAAAAAAECAwQF/8QAIxEBAQACAQQCAgMAAAAAAAAAAAECEQMSITFRBEETUhQyYf/aAAwDAQACEQMRAD8A+fgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA+qQ4H6Ca22Pxp94z5m6B8g+NU7wTb5QD6yuBmgPIPjVO8TzM0B5B8ap3gbfJQfW+ZfB/zf8ap3hzL4P8Am/41TvA2+SA+ucy+D/m/41TvDmVwf83/ABqneBt8jB9d5lcHvN/xqneHMrg95v8AjVO8Db5ED69zJ4Peb/jVO8OZPB7zf8ap3gbfIQfX+ZPB7zf8ap3hzJ4Peb/jVO8Db5AD6/zJ4Peb/jVO8OZPB7zf8ap3gr5AD6/zJ4Peb/jVO8OZPB7zf8ap3gPkAPr/ADJ4Peb/AI1TvDmTwe83/Gqd4D5AD6/zJ4Peb/jVO8OZPB7zf8ap3gPkAPr/ADJ4Peb/AI1TvDmTwe83/Gqd4D5AD6/zJ4Peb/jVO8OZPB7zf8ap3gm3yAH1/mTwe83/ABqneI5k8HvN/wAap3gbfIQfXuZPB7zf8ap3hzJ4Peb/AI1TvA2+Qg+u8yuD3m/41TvDmVwf83/Gqd4G3yIH1zmVwe83/Gqd4cy+D/m/41TvA2+Rg+ucy+D/AJv+NU7xHMvg/wCQfGqd4G3yQH1p8DOD/kHxqneI5maA8g+NU7wNvkwPrD4G6A8g+NU7xg+B2gcP+wfGqd4G3yoF52cMvDwhyKPWFUQXuRQ+8SrGL6X7CbXVUAdNaNj+1PHqMZaPpp7Jv2DcNVzgdDkEPvP2GyjouNWeqpteobhquWDufQdLLTr4x6CvV0XGnPV18+nBJlKtxscsHUqaK4vxm8P0GvkEfve4u4mq54OgrCGV4T9hl9H0v7x+wbhquaDocgp58d47A7CGMqT2egbNOeC7yOPWOSR6ymlIFzkkesxnbxjHIR9nprwUbUjCkvBRtQYEicEgKkYJQAYBIAYJAAAEhUAkAAAAAAUAAAABAAAAABAJAEAAIEEgDHAJIAhkGTICMWiMGbICtckYyXgvsNjRjJeCyo+QY2kodJKMV1jKCTe027YrwY4NSMlJrcyNDb6SDYpRl4y9hDhFvZL2k2rAu6OXhSKvFS6NvYbrOfFVlrbEyZd4Ty21ac6lWo9yRVnnO3oOnKEuOk0swljJWubZxblrRx0GccmrG6cVW0epdKNNlbRqQnKa2I36O+soVKTNkY8RYt7mzNutxdb7uXTocbW1IvG9omra1KcdbGV6Dfo/bdLsZZpuTuJ05LMdpu5WVmSOPgReHtL1KjHlcoNJoyna0p60YbJovVE6XOnHEvwMGjc1huEtmPca5LVZuVlrZrrfZs2s1V/s2VK+y0/FRsMKe5GwrkIkIkKEgAUpV5cp4xa/FKXF7vB7fbsCiuTVKzqTjNOb1tZ7MN9Bc1I6mpqrVxjHQYK2oqWtxUc5zu6QK6qTf1Em1OTUs52qO9/IilXm62vJT4urlLK2Lq9vyLsqcJPMop7MbugOEZRw4prqCqEatSnYR4ybevTTjNvbnG5m6UKcLiUpOajGGs/DfX2ll04OnxbgnDGNXGwOnCW2UUwKCr1qcJ6+vFzWsnJeLt247Ft9ptrqFCm9V1daUZJS121nDfX6C3KMZYyk8bUa1bUU9lKPVuCototQ2wlHKW+etkQlLjK+NuJpJN7tiM6dKFPOpFLJLpwecxW15fpYFe4ThW42etKkkvFk1qvO/HSRGlFXVVZniMItLXe/wvT6CxOjTnNTlCLktzaMtWOW8LLWGwObTqVadJPwoZpxfhT1s7VmW3qRZqQVDi5U5TcnNLDm3rJvb7tvqLHFwxFaqxHds3GMKFKnLWhTin6FuApqrUpWUnUm3GUG4zb2xfV8jbFca6rqOcnB4UYyawsdpZdODpum4pwaw4tbMEToUqjzOCb3ZA01KmbWnKEpRjPVWs96TNdzq0YzhDjVKUcp67w9q9Jc1IqGpqrVxjGNmDCNtRjnFOO3ZuAqxqzpRrLElPKUIOWs8vp7PkRGUnTjRm6mY1IrMm05Rf8AXuLrpwc1NxTlHc8biJ0qdTx4KXaBUrt0ampTnLVcctOTePCS9+WY+Hye4niaf1mJ8Y+t9BdjRpxi1GEUnv2byOT0k2+LW3OfTneBWqVJwjClUk9Zzjqy3ayyveZ0aUVc1ds/BxjM2+jtLEoRljWinh5WehkqKTbS2veEVWpSuqmxtRS267WNnURry5Fbtya11BSnnasr+vaW9VZbwsvea40KUE1GnFJrGOjAFevTdNwjRnJSnlYcm+jOfbgw5S5SVxrONGGIzXpe9+rZ7y5CjTptuEFFvpQ4uGq46q1XnKxsYRhb6zpKU860vCw+jPQbCQBDIJICMWYy8V9hmzGXivsKr5B0koglIxXWJJAIoCQAUmjYqslvee01jeQX6N1KnlSjlPBbV3QqLVqRx2o58aji3rQ6jXWuaK1ceN04Ri4yt706Fuo0brMJJ05encZ6TkuKjGO7ec5VqVVPi3h57DdqzUMqT3Zwya77N9tMdG/rcexlqreRpSlHV2p7zTSThKFSGrJ4baRrqRVeUp4ktu0WS03ZNRNpPjLtt9JajQSqyq63qKlpq066k5bCxSm3dyw8xlsyMljmXElKvKUdzZrzlYlu/A3XdPi7ia6M7CuzrHOkouLNFf7NlhSxszsNVylxTafqDNfYqfio2mFNbEbDbkEgBUgAASAFAAFAAAAJAAACCQAAAAAAAAAIAAAAAASQABJAAgkBEEGRARDIJAGLMZLwX2GbIl4r7APjxKGCcGK6hJBIUAAAlEEkGadSWVBSlJrcllnR0Vo63q2ylVjrSlvx+yUrOu7a4jUis9GO09NRjSp2a4yMXuxk1J2bwm64+k9CUrSnx9CbWrtcW96KMZSawpdB3NMulTslJSlmo8audh53Jjyuepey9Qg6UVUefCi8I2UJYoRx0y2lCF1Vg4pS3JmdG6dPKaym8mbjUlixTjBXc4SjlPcaqEoxqyUpOK9BhTr/ANp4yW7Ipas7na0o5Gk2m98GqlJ6+zsKz4t9DRlcz4ytKXsNLNydmbWWrB7pe00XMWqL7TYarh/VMqV9mp7kbDCn4qMzbkEgkAAAoAAoAAAAAAiUlGLlJpJb2ytyipX2WsPB/vZrwfUun8ALMpxhFynJRit7bwV+Wqf6vSqVv+pLEfa/yMXb0KbVW6qcZJbdaq9i7FuRly+2/Zm5/wCHBy/BAMXs+mjR7E5v8hyaq/HvK3+VRX5DlsOilX/hS+Q5fRXjKrH96lJfkA5Eum4uH/5GOSSXi3Vwv8yf4o2UrqhVeKdaEn1KW0mrXo0ftasIfvPAGrirqPiXMZeipT/NYHH3NP7W21l10pZ9zx+Y5dRb8BVZ/u0pNe3A5Z/9e4/0AZ0rqjWlqwn4fTCWyS9TNxSq3FpVWrcUp7NznRls9eNhFJyW2zuYVor/AIdSWX7d/tyBeBopXUJz4ucZUqv3J9PY9zN4AAASAAIBIAgEkAAAEQCSAiGYy8V9hmYy8R9gHySKzFE6plBeAuwywc3Zr1CNQ3apGqTatOoQ4s36pjgo0jDe42tBTw8dBrHG1L2ZW1OKuaUarwpNM9VO3VOk5Rw6ajlp9B5OrmpSUfu7n1FyOl60tFztqu2TjiMuvtNZTXhrCz7Vru5lczzLCivFityNGXjGTPVTpx6JdJEacmYvZO9asrK2bfQMrJujbtyWX7C1U0RUbap1KcupOWJE6o10Zac8jJZno69pRzK2m0uraV3GUfGi0/SsFYss8sWQySGEQabj7Jm5mq4+yZUr7RT8VGZhT8VGZpzSAAoAAoAAAAAGqvcRo4jhzqS8WEd7/rrNN7ext04RceM6W90e38l0lChKpXcuKVSet40k8OXbLcl6FtAsylxlX6/NxVW6hT8WHa92e31I38Vc1vtKqox+5S2v2v8AJGNO2uFDVVSnQh0Rowy1638jNWefHuLiT/fx+GAMqdlb03rcWpS+9Pwn7Wb1sKzs8eLcXEX16+fxya60ri0p67uIVIroqRxJvqTXyAuled2td06EXWqLeo7o9r/plCrfa09W+17SDWY01tc/WvwLdN13BRt7eFCmtzqd1fMCKli7v9clFr7lNYXt3+zAjo2nQm52knSk9+fDT9u32M2cnuJePeTT/wC3CKXvTHJJ+WXHtj8gI5RVo/rNLwf7yn4S9a3osU5wqQU4SUovc08o0cRdQX1d1r/4tNP8MFG4rStqzxTcK7WcUPDUv3o7/X7wOuaqttRrfaUoSfXjavWULa+r3klBypW7f+dy68Pdn0by3yPWX1lxXn/n1f8A84A11bGThq06jlD7lbwl6nvXtNEbutZSUbmE3T6G9rXZLp9e3tLasKS3Trp/40vmRK1rKLULmUk/2a0VJfk/eBYpVYVqanTkpRfSjI4VWhd6PqOvbQ1F+3TTcqcvzi/cdLR+kaN/TzTerUj49N74/wC3pAuAgkAAAAAAgAAAAEQzGXiPsMzGfiPsCPlNPxI9hmjCn4kewzRxvl6InBGDIE2rHBGqZkMqaapQk1iMW+xZNM9iz1HV0fFzuYR6E8m3hNOgoUIwhFVMvWeNp2wzk7JcLZ1ONGeTDKWV6TFSSN1O2dR5nsXUayykTHG3wwc51Hq012stQXg7TJQjHYlhIlI8+WfU9GGHShLajs1Kes9yew5MYp4OxXerTcktqjn3HKuuLQoThti5R/dlgSnVeyUtZdU4pm3GVvJwF0ozo0Zt69tTfpi3Eq1rK2UXL6ymvQ1JHWcE+gq3lNK3n2FmVZuEca6tlRinGbkm+rBSuPsmde/X1VP1fgjlXUcUWdsbt5OSavZ9np+KZmEPFRmdHEAAUAAAAADn6S0jyeUbehF1Lqfiwjtx6TPSN7yaKp0dV15rZrPCgvvP0FbR9rOkpToRzUqbalzXW2fZHq9gEWeiHKSrX8+MnnKpJ+Cu3rZ1opRSUUkluSK/I9bbWr1qj/f1V7FgcgtvuNenXfzAsklXktSntoXFSP8A01Hrr37feaLrSfIaf9qpNVHsgobVUfUur1gWrq6hbU9aW2T8WOd5So07i6nx0panVPG5dUE93a95ot4zrVnWuIO4uG/s4+JT6k3u2dR0eKu6n2leNJfdpRz738gNlK1o0otRgnreM5bXLtfSanQqW3hWrzDpoyez/K+j8CeR533Fw3+/j8ByatH7K6n2VEpL8n7wNtCvCvFuOU1slF7HF9TNjaSbbwkc64lUpvja0FSnFbK9PbHHVJb8e3tNML6N80qkZNLDVvDa6n/U/wDp6vf1AXOMq3bxQbp0emrjbL935m+jQp0I6tOOM7W+lv0vpNKjd1d84UI9UVrS9r2e4nkafj3FxJ/4jX4YAi7sKN0m5Jwn9+OxlaldXFhNUdINTpN4hcrd2S6u0tckcfs7mvHtnrfjkxmrqEXGcKdzTexrGrJrs3P3AW96ygcqlc07HOJPky8anPZKj6umJajWr3KUreKp0nuqTWW+yPzAtnOvNE061VXNtLk91HaqkVsfaukscjUvta9eo/33FexYHIaP7M60X1qrL5gYWd5Oc+T3cFSuYrcvFmuuJcKFzZ1p08Kpxyi8x1vBnF9akvkRY6Q4yq7W5zC5ispSWNddYHRBBIAAAQCQBAAAGM/EfYZET8SXYEfJ4eIuwzRrp+Kuw2I413jJEmJOSKkgkgDda1OKrKWcek6N7G3vrRyqrwoJuM1vRyCvUrShV1Yzks71nYzWM35WZWdk0qUYvO9lmGxGpbEbo7jnba7SaY9YWQSkRUx3o7NdZt5YW3U/I5EPGOzV1dRJzdN4W1ErWKk5VE5JRzPWT8H0JdZlG4Udm2W1+rq+RthTqRcmq8JZfTH0egSpz2/V0nl5ym1+Q7HdqjcOanHMdZNLZ1ZwY3D1rOo31YNjg5KEXRmtXc4tMxuFi0qLDWx7wObeLNCn2L8Dl3n2Eu1HWu1/Z6b9C/A5V7+ry9R1weXl8vskNxkYw3IyOzzhJBIVW0jVnQ0fcVabxOFOUovGdqR4mjp/Tlwvqrqm3hvHFx+R7PSqb0XdpLLdKWF6jxuiY01CDnHDzteN205c3JcMZY68PFOTPVdDRemdLwvqUb9Rq0arUcKKi4tvCftO/pHSdOylCioudxV2UoL9p+l9BxbiFN3NpOnDOa0NuM4Wuuk7F7omnd39C7dWpCpS2LGMY/pk4c8s5eo5eOYWSIsdGunN3F5Pjrmby3+zH0Jeg6JW5JLyu49q+RlC2lCak7mtLHRJrD9x2cnnNe8uLyvCnc1Vqye+tJLBsVDScZKULqWsnla1aTXrWNpFkmtJ3aknFpvY1jqOmfG5/k8nHyWSvRjhLNts9K0Lewp3N21Tc08RWXl9SKdro+vpGu7zSS1IyWKdH7sep9RlLRVPSdjaOpUnB0m2sbt50OSS8ruPavkfYxu5K89a4r6OSj/ym5Ppp9vo/A4/C6+urN23Ja8qespZ1Xv3HcdnJpp3Vdp9DcfkeZ4X2zoW9nGHGTp01Ja0tuN2FkXw68MlzkrkfS2lvLqn+o9FwY0vcXCq0LySnxcXPjc7cek8odzgpTjWu7mnPOrOi4vDxsyjz8fJblqvsfM+JxcfDc8Z3dzlMtL1nC0f9lg/CqtbJP8AP+vXnW0W6GK2jpaleO1qT2VOvPaTYaGhYU506NzXUJS1sLHyLXJJeV3HtXyPS+EWd2rqi3qunUhsqU5b4M8bTv8ASdZvUvJrGN8mexhYxp3DuONqyqauq9ZravThHi7GMozqxnFxksJqSw1vN4xx5bZNxaoXulqNaFSVyqkYvLhKTakj1PL6So0nL7WrBSjSjtk89R5d+K+w7a0TTu1Y3fHVKdWlSglqvZjAymk4c7lva27FXLVW88KovEUXhU+z0+krxlW0VW1avh2Un9ol9m/Suouckl5Xce1fIOzbTTuq7T6G4/Iw7vO8N6s4KydKpKKlr+K8Z3Hl3O48pq/6n8z03C+wnSsrPiYznSouScm86ucY/A80950x8MZPUcDr65nUqWlapxlOEdeLlvW3d7z0N5ZUryCU8xnF5hUjslB9aZ47g3au7ubilGvUot0vGpvbvR7CNnJRS5XcbFjevkZy8tTwi0uZ8Y7a6wriKymt1Rda/NFlzipKLzl9SK0rBTlCUriu5QeYvMdnuN06LlOL1lsWNqMq25XWRKSjFybwkss0O1jjZq52b45RnToKnCS2Nvpa9AGyM1JZWfWsE5XWU6ltNJY8PrXQvaZO02RUZJJYzs3tdIFqLUoprantRJjTjqQjHfhYMgIIn4kuwkifiS7APksH4KM4s1Q8VGyLOVdY2EmCZkiKkgkgihVrL65dqLRQl+t/5jeH2zV5GxbjXE2x3HKvSxWTKIRMcEGS8ZYO/UowaTcU9i/A4KXhI9BVnFT1ZSknhboNr3Ga1FaVCEk0o+wwdun4spR7Dc6lNf8AEiu3K/ERcJbpwfZNBVeVKpt1Z46DRcqrxM02mknll2S2bMsrXWeJnjOcMTyVzLv9VpdiOTefq77UdW6/U6fYvzOTefYPtO+Dy8vl9mh4qMjGHioyOrzwJACoKEtDWbbahOOW3iM2lt9BfJJZtZbPClDRdvDUxxuINNJ1HjY8ouEga0W2+UAkFRXrWVvXqcZUg9fGNaMnF49Rr+jbb7tT+LL5lwGbjL5hthSpQo0406axGOxLJmAaA1XFvSuqMqNeCnCSw0zaAOTzc0X5O/8AXL5liy0VZ2FSVS2pOEpLDes3s9ZeBNRu8ueU1bQAFYQU7jRVpc1nVqU3rtYbjJxz7C6Alm/Lm/Qdj9yp/Fl8zoU4Rp04wgsRikkvQZAEkngAAVhUhCrBwqRUoyWGmsplb6K0f5Fb/wANFwgDRQsrW2k5ULelSk1huEUjeAAAAAAAACQAAAgNZWGABwObeivJf55fMnm5oryb+eXzOqAOVzd0X5N/PL5k83tGeT/zy+Z1ANQ25n0Bo3GOTfzP5j6A0Z5N/M/mdMDUXdcz6A0Z5N/O/mYc29EuWtyRZ351pfM6pGtHONZZ6sjUNud9AaM8m/mfzJ+gdG+T/wAz+Z0gTUXqvtzfoHRvk/8AM/mFoLRy/wCX/mfzLV3eW9lSdS5qxpxXW9r7Ecarww0fCL4unWqSzsWqkmXpno677dD6D0ev+X/mfzLKs6CedT3lLRGnbbSspU4J06scvUl0rrOoTpno68vbQ7K3lvpp+swejbSW+jF9pbA6cfR15e1F6HsH/wAvH1Noj6Hsv7qS/wA8vmXwOmejry9udU0Ho6qsTt9ZemT+ZpnwZ0RNYlaJr96XzOuC6iW2t8NyMjGHioyDMSAAqCSABIIAEggASCABIIAEg11Z8XDW9K/E18pX93Lfjo3/ANMCwCurnwsar9C6cmUKspznFR2xW9+v5AbgV53Dg8ODeHhtdn/ocqWccXU9nT1AWAauOWopar34foMZXMVjEZPwdZ46EBvBojcKTxGnP0bMZQ5THOxNrZtA3grq5TS8GTzsT2bWbISlPEtij1dP9bwNhBJAAAAAAAAAAkgkAAAIHQABWANdevSt6bqVqkYQXTJ4KNhQ0tpSjoq2VWqnKUniEFvbOJpPhPV4xw0ekoL/AIkllv1HnrircXdTXr1Z1JN/tPcamFYucdiHCvSVapKNG1pzb8WMYttfMrX2mNO0561eVS2U90VDVXvO5W1ODmiKUbalGVzVaTk1veNr/wBjgaUr6Rq1NS/eZYTSxhJPaWTZctKtatpS+1ricripFb5RT1Vjs2IpwdXjVKnKfGZ2OLecnq+B9df2iyqeLJa8V7n+RU0Popx4RypTWY20nLb6N35FTe3Ir1tJ0VGNerd087YqcpL2ZN6jpycNnL3FrG+e4tcJbh3mlppPMKPgR7en3nodO3N9b2ts7Bz1m8S1Ya2zHYDbwtxGuqmLlVFP/uZz7zKjZXNxHWoW9WpHdmMG0ev4QQdfg9Rq3kIxu049uelewcH+Np8G67oZ41ObhhZecLA32X708pGlfaOrQuOJrUZQeVKUGkes0VwqoXPgXqjbzW6WfBl8jfoqrezsbh6aSVLGx1IqLa6dh4qdNa71V4OdnYNbS3T6bSq061NVKU4zg90ovKMz5vZ3d3Yy1ratKHWt6fqPR2PCpNat7R1X9+n0+ozcKszj0oOba6csLptRrKm1/eeDkvwq05w14VISj1p5RnVa3GYIJIrdB+CjIwpvwUZhlIADQAAAAAAAAAAAAANJrDWUYzpwnHDisZTMgBjxcMY1I47CVCKeVFJ7iQBi6cJPLjFvdloiVGnJY1F7DMAY8XDVUdVYW5YMeIpaylqRyvQbABChCLbUUm+pEcXB/srr3GQAxVOCllRjnrwZLC3AAAAAAAAAAAAAJIJAAgAAQwEcbSGkZ0INW1GVSed7i8I8ve1K95WdSvJuT/Z6F2I9yYuEXvivYdcc8cfpx5OPPLxk8CraT3Rb9RlySotupJeo95qx+6vYNSH3V7Df5cfTn/Hy/ZzJpaWsaUqU1TuKTUsSW59XYV6+gncxU6rjxr2yVPwVJ+vP4HVrTdKouLpayxmWF6V/uYU7ms6sYyota72dGqtVN59rOfV6d+jf9nmaFvW0XpOnUnTcYxlh9Kx07T0deFOz5VewXhzgvatxsc6sqk0oJxjOKWYNZTe3/wBkV61aMsU6Wt4DeHF4z0bV+AuW2ccLjvu8U6LlJuWW28s9Vpi8uLO3oO3aTlseVnoNl5C5nbQrRuoWcYwcqjdNS2+vcUvpa6jwchdzhHlE5akG44W/Y8G7lLZdMY8eWMs249zK8vpqVdzqY3LV2L1Hb0RCrQ0FX1VKNROTjs25wbbG4uaOlp6Puqyr5pKrGeqotda2GNrc3q0/O0uKtOVPieMUYRwltwhllLNSLhhZd2o0dKppPR9e2vfCmtzksP0M81Us6lKbhODUl0NHpNNzvLOMq9G+1deSjToqim230ZOrbRqK2pq4anV1VryxvZJnMe+jLiyykm+8eFVrOW6DfYjbHRlzLxaFR9kWe6wuokv5p6Znx795PC/RdznHETz+6yXo27itXiamP3We4BPy/wCL/Hv7PLWM9L2i1adOpKH3akW0j0NrdTrQXG0KlKfSmthZBjLKX6dcMLj9sqb8FGzJ5yHDPQCW2/8Ag1O6Zc9OD/nD4NTumGnokycnnlw14P8AnD4NTuk89eD3nD4NTugehB57ntwe84fBqd0nntwe84fBqd0K9CDz3Pbg95w+DU7o57cHvOHwandCvQg8/wA9+D3nD4NTujnvwe84fBqd0D0APP8APfg95w+DU7o578HvOHwandA9ADz/AD34PecPg1O6Oe/B7zh8Gp3QPQA8/wA9uDvnD4NTujntwd84fBqd0D0APP8APbg75w+DU7o57cHfOHwandA9ADz/AD24O+cPg1O6Oe3B3zh8Gp3QPQA8/wA9uDvnD4NTujntwd84fBqd0Ds17mFCUVNPDWW10f1k1vSFulvl/pZyXw14ON5d+m/8Cp3TF8MuDTkpcujlbvqKndA7avKOpGTk0pPV2rpMVpC3bxrSzt/ZZxlwz4NqKSvlhf8AYqd0R4ZcGo7r5Lbn7Cp3QOy7+3T2yf8ApZEr+lGHGYk4a2rnHoznsORz04N+XL+BU7pPPTg5j9fX8Cp3QOzSu6VV6sdbPTlbjBX9LV1p5jnoxnoT6O05HPPg35cv4FTukLhlwaSSV8tmz7Cp3QO3VuoUlTbTaqbv9wrunKhKrHLSeN2NucHF558G0sK+WP8AAqd0c8+DeX/blt3/AFFTb/KB1XpGgsbW3szjoJekbdPfLGMt6r2HJ558G/Ll/Aqd0xjww4MxcnG9Sct/1FTugdyV1T4iVWOWo7MPZlmtaQo7p60ZZa1cZ3HI558G9v8Abo7f+xU7pPPPg35cv4FTugdnllOVGpUp5nqLLWMZIje05Zwm8Lbjr2bPecSHDDgzTcnC9Scnl/UVNv8AKHwx4N5b5csvf9TU7oR1/pCDk46ks4zsa68GyncxqVJU1FpptezHzOJzy4OLdfJf+Cp3SFwy4Op5V8k/8Gp3QjuA+eVP/kC8VSSp0beUE3qvEtq9pj+kC/8A7i39kvmGn0UHzr9IF/8A3Fv7JfMfpAv/ACe39kvmB9ClDWlnWa2YwjFUmnF68vBz6z5/+kC/8nt/ZL5j9IN/5Pb+yXzLtNR9BjTcUlrt4yFTajhzb2NZPn36QL7ye39kvmP0g3/k9v7JfMbNR7LSeip6RjTjK7nThDa4qKak+tmc9Gcfo6VpdV5VcvMZ6qi443YSPFfpBv8Aye39kvmP0g3/AJPb+yXzL1U1HtbLRrt7mdzWuJXFecVDWlFLEV0YNisEtKu/4x6zpcXqY2b85PDfpBv/ACe39kvmbqHD6vJS4+nRhjdqwbz7+wm6aezrWCr6Qo3VSo3GinqU8bE+suHg58PZcZTUFTcHra7dN5XVjb0k1uHso54lU57NmtTa/MK92DwVPh7VlTi6ipQnjalSb/PsMqXD2Tt26vFxrY2RVNtZ7c9hB7sHg48PpPjNaMVjGolTe33mEuH1fiNaNOi6v3XB439eeoD34PDc/FrQ2w1dus1SezqxtMY8PJca1Li9TKw1TeXv9PYB4QAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAH//2Q==\n",
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"600\"\n",
              "            height=\"300\"\n",
              "            src=\"https://www.youtube.com/embed/-W6y8xnd--U\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "        ></iframe>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.lib.display.YouTubeVideo at 0x1ec58acc2e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1kFh7pdxhNtT",
        "outputId": "d078719d-984a-4e53-8c93-3f297f8d84b4",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 9945
        }
      },
      "source": [
        "model = Model(inputs=[input], outputs=[output])\n",
        "model.summary()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_4 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_159 (Conv2D)             (None, 32, 32, 20)   540         input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_156 (BatchN (None, 32, 32, 20)   80          conv2d_159[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_159 (Activation)     (None, 32, 32, 20)   0           batch_normalization_156[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_160 (Conv2D)             (None, 32, 32, 18)   3240        activation_159[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_153 (Dropout)           (None, 32, 32, 18)   0           conv2d_160[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_144 (Concatenate)   (None, 32, 32, 38)   0           conv2d_159[0][0]                 \n",
            "                                                                 dropout_153[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_157 (BatchN (None, 32, 32, 38)   152         concatenate_144[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_160 (Activation)     (None, 32, 32, 38)   0           batch_normalization_157[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_161 (Conv2D)             (None, 32, 32, 18)   6156        activation_160[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_154 (Dropout)           (None, 32, 32, 18)   0           conv2d_161[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_145 (Concatenate)   (None, 32, 32, 56)   0           concatenate_144[0][0]            \n",
            "                                                                 dropout_154[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_158 (BatchN (None, 32, 32, 56)   224         concatenate_145[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_161 (Activation)     (None, 32, 32, 56)   0           batch_normalization_158[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_162 (Conv2D)             (None, 32, 32, 18)   9072        activation_161[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_155 (Dropout)           (None, 32, 32, 18)   0           conv2d_162[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_146 (Concatenate)   (None, 32, 32, 74)   0           concatenate_145[0][0]            \n",
            "                                                                 dropout_155[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_159 (BatchN (None, 32, 32, 74)   296         concatenate_146[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_162 (Activation)     (None, 32, 32, 74)   0           batch_normalization_159[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_163 (Conv2D)             (None, 32, 32, 18)   11988       activation_162[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_156 (Dropout)           (None, 32, 32, 18)   0           conv2d_163[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_147 (Concatenate)   (None, 32, 32, 92)   0           concatenate_146[0][0]            \n",
            "                                                                 dropout_156[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_160 (BatchN (None, 32, 32, 92)   368         concatenate_147[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_163 (Activation)     (None, 32, 32, 92)   0           batch_normalization_160[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_164 (Conv2D)             (None, 32, 32, 18)   14904       activation_163[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_157 (Dropout)           (None, 32, 32, 18)   0           conv2d_164[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_148 (Concatenate)   (None, 32, 32, 110)  0           concatenate_147[0][0]            \n",
            "                                                                 dropout_157[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_161 (BatchN (None, 32, 32, 110)  440         concatenate_148[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_164 (Activation)     (None, 32, 32, 110)  0           batch_normalization_161[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_165 (Conv2D)             (None, 32, 32, 18)   17820       activation_164[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_158 (Dropout)           (None, 32, 32, 18)   0           conv2d_165[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_149 (Concatenate)   (None, 32, 32, 128)  0           concatenate_148[0][0]            \n",
            "                                                                 dropout_158[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_162 (BatchN (None, 32, 32, 128)  512         concatenate_149[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_165 (Activation)     (None, 32, 32, 128)  0           batch_normalization_162[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_166 (Conv2D)             (None, 32, 32, 18)   20736       activation_165[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_159 (Dropout)           (None, 32, 32, 18)   0           conv2d_166[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_150 (Concatenate)   (None, 32, 32, 146)  0           concatenate_149[0][0]            \n",
            "                                                                 dropout_159[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_163 (BatchN (None, 32, 32, 146)  584         concatenate_150[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_166 (Activation)     (None, 32, 32, 146)  0           batch_normalization_163[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_167 (Conv2D)             (None, 32, 32, 18)   23652       activation_166[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_160 (Dropout)           (None, 32, 32, 18)   0           conv2d_167[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_151 (Concatenate)   (None, 32, 32, 164)  0           concatenate_150[0][0]            \n",
            "                                                                 dropout_160[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_164 (BatchN (None, 32, 32, 164)  656         concatenate_151[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_167 (Activation)     (None, 32, 32, 164)  0           batch_normalization_164[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_168 (Conv2D)             (None, 32, 32, 18)   26568       activation_167[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_161 (Dropout)           (None, 32, 32, 18)   0           conv2d_168[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_152 (Concatenate)   (None, 32, 32, 182)  0           concatenate_151[0][0]            \n",
            "                                                                 dropout_161[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_165 (BatchN (None, 32, 32, 182)  728         concatenate_152[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_168 (Activation)     (None, 32, 32, 182)  0           batch_normalization_165[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_169 (Conv2D)             (None, 32, 32, 18)   29484       activation_168[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_162 (Dropout)           (None, 32, 32, 18)   0           conv2d_169[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_153 (Concatenate)   (None, 32, 32, 200)  0           concatenate_152[0][0]            \n",
            "                                                                 dropout_162[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_166 (BatchN (None, 32, 32, 200)  800         concatenate_153[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_169 (Activation)     (None, 32, 32, 200)  0           batch_normalization_166[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_170 (Conv2D)             (None, 32, 32, 18)   32400       activation_169[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_163 (Dropout)           (None, 32, 32, 18)   0           conv2d_170[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_154 (Concatenate)   (None, 32, 32, 218)  0           concatenate_153[0][0]            \n",
            "                                                                 dropout_163[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_167 (BatchN (None, 32, 32, 218)  872         concatenate_154[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_170 (Activation)     (None, 32, 32, 218)  0           batch_normalization_167[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_171 (Conv2D)             (None, 32, 32, 18)   35316       activation_170[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_164 (Dropout)           (None, 32, 32, 18)   0           conv2d_171[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_155 (Concatenate)   (None, 32, 32, 236)  0           concatenate_154[0][0]            \n",
            "                                                                 dropout_164[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_168 (BatchN (None, 32, 32, 236)  944         concatenate_155[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_171 (Activation)     (None, 32, 32, 236)  0           batch_normalization_168[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_172 (Conv2D)             (None, 32, 32, 18)   4248        activation_171[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_165 (Dropout)           (None, 32, 32, 18)   0           conv2d_172[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_15 (AveragePo (None, 16, 16, 18)   0           dropout_165[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_169 (BatchN (None, 16, 16, 18)   72          average_pooling2d_15[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "activation_172 (Activation)     (None, 16, 16, 18)   0           batch_normalization_169[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_173 (Conv2D)             (None, 16, 16, 18)   2916        activation_172[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_166 (Dropout)           (None, 16, 16, 18)   0           conv2d_173[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_156 (Concatenate)   (None, 16, 16, 36)   0           average_pooling2d_15[0][0]       \n",
            "                                                                 dropout_166[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_170 (BatchN (None, 16, 16, 36)   144         concatenate_156[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_173 (Activation)     (None, 16, 16, 36)   0           batch_normalization_170[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_174 (Conv2D)             (None, 16, 16, 18)   5832        activation_173[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_167 (Dropout)           (None, 16, 16, 18)   0           conv2d_174[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_157 (Concatenate)   (None, 16, 16, 54)   0           concatenate_156[0][0]            \n",
            "                                                                 dropout_167[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_171 (BatchN (None, 16, 16, 54)   216         concatenate_157[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_174 (Activation)     (None, 16, 16, 54)   0           batch_normalization_171[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_175 (Conv2D)             (None, 16, 16, 18)   8748        activation_174[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_168 (Dropout)           (None, 16, 16, 18)   0           conv2d_175[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_158 (Concatenate)   (None, 16, 16, 72)   0           concatenate_157[0][0]            \n",
            "                                                                 dropout_168[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_172 (BatchN (None, 16, 16, 72)   288         concatenate_158[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_175 (Activation)     (None, 16, 16, 72)   0           batch_normalization_172[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_176 (Conv2D)             (None, 16, 16, 18)   11664       activation_175[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_169 (Dropout)           (None, 16, 16, 18)   0           conv2d_176[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_159 (Concatenate)   (None, 16, 16, 90)   0           concatenate_158[0][0]            \n",
            "                                                                 dropout_169[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_173 (BatchN (None, 16, 16, 90)   360         concatenate_159[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_176 (Activation)     (None, 16, 16, 90)   0           batch_normalization_173[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_177 (Conv2D)             (None, 16, 16, 18)   14580       activation_176[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_170 (Dropout)           (None, 16, 16, 18)   0           conv2d_177[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_160 (Concatenate)   (None, 16, 16, 108)  0           concatenate_159[0][0]            \n",
            "                                                                 dropout_170[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_174 (BatchN (None, 16, 16, 108)  432         concatenate_160[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_177 (Activation)     (None, 16, 16, 108)  0           batch_normalization_174[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_178 (Conv2D)             (None, 16, 16, 18)   17496       activation_177[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_171 (Dropout)           (None, 16, 16, 18)   0           conv2d_178[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_161 (Concatenate)   (None, 16, 16, 126)  0           concatenate_160[0][0]            \n",
            "                                                                 dropout_171[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_175 (BatchN (None, 16, 16, 126)  504         concatenate_161[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_178 (Activation)     (None, 16, 16, 126)  0           batch_normalization_175[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_179 (Conv2D)             (None, 16, 16, 18)   20412       activation_178[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_172 (Dropout)           (None, 16, 16, 18)   0           conv2d_179[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_162 (Concatenate)   (None, 16, 16, 144)  0           concatenate_161[0][0]            \n",
            "                                                                 dropout_172[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_176 (BatchN (None, 16, 16, 144)  576         concatenate_162[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_179 (Activation)     (None, 16, 16, 144)  0           batch_normalization_176[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_180 (Conv2D)             (None, 16, 16, 18)   23328       activation_179[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_173 (Dropout)           (None, 16, 16, 18)   0           conv2d_180[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_163 (Concatenate)   (None, 16, 16, 162)  0           concatenate_162[0][0]            \n",
            "                                                                 dropout_173[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_177 (BatchN (None, 16, 16, 162)  648         concatenate_163[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_180 (Activation)     (None, 16, 16, 162)  0           batch_normalization_177[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_181 (Conv2D)             (None, 16, 16, 18)   26244       activation_180[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_174 (Dropout)           (None, 16, 16, 18)   0           conv2d_181[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_164 (Concatenate)   (None, 16, 16, 180)  0           concatenate_163[0][0]            \n",
            "                                                                 dropout_174[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_178 (BatchN (None, 16, 16, 180)  720         concatenate_164[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_181 (Activation)     (None, 16, 16, 180)  0           batch_normalization_178[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_182 (Conv2D)             (None, 16, 16, 18)   29160       activation_181[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_175 (Dropout)           (None, 16, 16, 18)   0           conv2d_182[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_165 (Concatenate)   (None, 16, 16, 198)  0           concatenate_164[0][0]            \n",
            "                                                                 dropout_175[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_179 (BatchN (None, 16, 16, 198)  792         concatenate_165[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_182 (Activation)     (None, 16, 16, 198)  0           batch_normalization_179[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_183 (Conv2D)             (None, 16, 16, 18)   32076       activation_182[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_176 (Dropout)           (None, 16, 16, 18)   0           conv2d_183[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_166 (Concatenate)   (None, 16, 16, 216)  0           concatenate_165[0][0]            \n",
            "                                                                 dropout_176[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_180 (BatchN (None, 16, 16, 216)  864         concatenate_166[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_183 (Activation)     (None, 16, 16, 216)  0           batch_normalization_180[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_184 (Conv2D)             (None, 16, 16, 18)   34992       activation_183[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_177 (Dropout)           (None, 16, 16, 18)   0           conv2d_184[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_167 (Concatenate)   (None, 16, 16, 234)  0           concatenate_166[0][0]            \n",
            "                                                                 dropout_177[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_181 (BatchN (None, 16, 16, 234)  936         concatenate_167[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_184 (Activation)     (None, 16, 16, 234)  0           batch_normalization_181[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_185 (Conv2D)             (None, 16, 16, 18)   4212        activation_184[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_178 (Dropout)           (None, 16, 16, 18)   0           conv2d_185[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_16 (AveragePo (None, 8, 8, 18)     0           dropout_178[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_182 (BatchN (None, 8, 8, 18)     72          average_pooling2d_16[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "activation_185 (Activation)     (None, 8, 8, 18)     0           batch_normalization_182[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_186 (Conv2D)             (None, 8, 8, 18)     2916        activation_185[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_179 (Dropout)           (None, 8, 8, 18)     0           conv2d_186[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_168 (Concatenate)   (None, 8, 8, 36)     0           average_pooling2d_16[0][0]       \n",
            "                                                                 dropout_179[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_183 (BatchN (None, 8, 8, 36)     144         concatenate_168[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_186 (Activation)     (None, 8, 8, 36)     0           batch_normalization_183[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_187 (Conv2D)             (None, 8, 8, 18)     5832        activation_186[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_180 (Dropout)           (None, 8, 8, 18)     0           conv2d_187[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_169 (Concatenate)   (None, 8, 8, 54)     0           concatenate_168[0][0]            \n",
            "                                                                 dropout_180[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_184 (BatchN (None, 8, 8, 54)     216         concatenate_169[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_187 (Activation)     (None, 8, 8, 54)     0           batch_normalization_184[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_188 (Conv2D)             (None, 8, 8, 18)     8748        activation_187[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_181 (Dropout)           (None, 8, 8, 18)     0           conv2d_188[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_170 (Concatenate)   (None, 8, 8, 72)     0           concatenate_169[0][0]            \n",
            "                                                                 dropout_181[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_185 (BatchN (None, 8, 8, 72)     288         concatenate_170[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_188 (Activation)     (None, 8, 8, 72)     0           batch_normalization_185[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_189 (Conv2D)             (None, 8, 8, 18)     11664       activation_188[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_182 (Dropout)           (None, 8, 8, 18)     0           conv2d_189[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_171 (Concatenate)   (None, 8, 8, 90)     0           concatenate_170[0][0]            \n",
            "                                                                 dropout_182[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_186 (BatchN (None, 8, 8, 90)     360         concatenate_171[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_189 (Activation)     (None, 8, 8, 90)     0           batch_normalization_186[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_190 (Conv2D)             (None, 8, 8, 18)     14580       activation_189[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_183 (Dropout)           (None, 8, 8, 18)     0           conv2d_190[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_172 (Concatenate)   (None, 8, 8, 108)    0           concatenate_171[0][0]            \n",
            "                                                                 dropout_183[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_187 (BatchN (None, 8, 8, 108)    432         concatenate_172[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_190 (Activation)     (None, 8, 8, 108)    0           batch_normalization_187[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_191 (Conv2D)             (None, 8, 8, 18)     17496       activation_190[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_184 (Dropout)           (None, 8, 8, 18)     0           conv2d_191[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_173 (Concatenate)   (None, 8, 8, 126)    0           concatenate_172[0][0]            \n",
            "                                                                 dropout_184[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_188 (BatchN (None, 8, 8, 126)    504         concatenate_173[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_191 (Activation)     (None, 8, 8, 126)    0           batch_normalization_188[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_192 (Conv2D)             (None, 8, 8, 18)     20412       activation_191[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_185 (Dropout)           (None, 8, 8, 18)     0           conv2d_192[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_174 (Concatenate)   (None, 8, 8, 144)    0           concatenate_173[0][0]            \n",
            "                                                                 dropout_185[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_189 (BatchN (None, 8, 8, 144)    576         concatenate_174[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_192 (Activation)     (None, 8, 8, 144)    0           batch_normalization_189[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_193 (Conv2D)             (None, 8, 8, 18)     23328       activation_192[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_186 (Dropout)           (None, 8, 8, 18)     0           conv2d_193[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_175 (Concatenate)   (None, 8, 8, 162)    0           concatenate_174[0][0]            \n",
            "                                                                 dropout_186[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_190 (BatchN (None, 8, 8, 162)    648         concatenate_175[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_193 (Activation)     (None, 8, 8, 162)    0           batch_normalization_190[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_194 (Conv2D)             (None, 8, 8, 18)     26244       activation_193[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_187 (Dropout)           (None, 8, 8, 18)     0           conv2d_194[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_176 (Concatenate)   (None, 8, 8, 180)    0           concatenate_175[0][0]            \n",
            "                                                                 dropout_187[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_191 (BatchN (None, 8, 8, 180)    720         concatenate_176[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_194 (Activation)     (None, 8, 8, 180)    0           batch_normalization_191[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_195 (Conv2D)             (None, 8, 8, 18)     29160       activation_194[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_188 (Dropout)           (None, 8, 8, 18)     0           conv2d_195[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_177 (Concatenate)   (None, 8, 8, 198)    0           concatenate_176[0][0]            \n",
            "                                                                 dropout_188[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_192 (BatchN (None, 8, 8, 198)    792         concatenate_177[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_195 (Activation)     (None, 8, 8, 198)    0           batch_normalization_192[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_196 (Conv2D)             (None, 8, 8, 18)     32076       activation_195[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_189 (Dropout)           (None, 8, 8, 18)     0           conv2d_196[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_178 (Concatenate)   (None, 8, 8, 216)    0           concatenate_177[0][0]            \n",
            "                                                                 dropout_189[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_193 (BatchN (None, 8, 8, 216)    864         concatenate_178[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_196 (Activation)     (None, 8, 8, 216)    0           batch_normalization_193[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_197 (Conv2D)             (None, 8, 8, 18)     34992       activation_196[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_190 (Dropout)           (None, 8, 8, 18)     0           conv2d_197[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_179 (Concatenate)   (None, 8, 8, 234)    0           concatenate_178[0][0]            \n",
            "                                                                 dropout_190[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_194 (BatchN (None, 8, 8, 234)    936         concatenate_179[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_197 (Activation)     (None, 8, 8, 234)    0           batch_normalization_194[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_198 (Conv2D)             (None, 8, 8, 18)     4212        activation_197[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_191 (Dropout)           (None, 8, 8, 18)     0           conv2d_198[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_17 (AveragePo (None, 4, 4, 18)     0           dropout_191[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_195 (BatchN (None, 4, 4, 18)     72          average_pooling2d_17[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "activation_198 (Activation)     (None, 4, 4, 18)     0           batch_normalization_195[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_199 (Conv2D)             (None, 4, 4, 18)     2916        activation_198[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_192 (Dropout)           (None, 4, 4, 18)     0           conv2d_199[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_180 (Concatenate)   (None, 4, 4, 36)     0           average_pooling2d_17[0][0]       \n",
            "                                                                 dropout_192[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_196 (BatchN (None, 4, 4, 36)     144         concatenate_180[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_199 (Activation)     (None, 4, 4, 36)     0           batch_normalization_196[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_200 (Conv2D)             (None, 4, 4, 18)     5832        activation_199[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_193 (Dropout)           (None, 4, 4, 18)     0           conv2d_200[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_181 (Concatenate)   (None, 4, 4, 54)     0           concatenate_180[0][0]            \n",
            "                                                                 dropout_193[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_197 (BatchN (None, 4, 4, 54)     216         concatenate_181[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_200 (Activation)     (None, 4, 4, 54)     0           batch_normalization_197[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_201 (Conv2D)             (None, 4, 4, 18)     8748        activation_200[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_194 (Dropout)           (None, 4, 4, 18)     0           conv2d_201[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_182 (Concatenate)   (None, 4, 4, 72)     0           concatenate_181[0][0]            \n",
            "                                                                 dropout_194[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_198 (BatchN (None, 4, 4, 72)     288         concatenate_182[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_201 (Activation)     (None, 4, 4, 72)     0           batch_normalization_198[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_202 (Conv2D)             (None, 4, 4, 18)     11664       activation_201[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_195 (Dropout)           (None, 4, 4, 18)     0           conv2d_202[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_183 (Concatenate)   (None, 4, 4, 90)     0           concatenate_182[0][0]            \n",
            "                                                                 dropout_195[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_199 (BatchN (None, 4, 4, 90)     360         concatenate_183[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_202 (Activation)     (None, 4, 4, 90)     0           batch_normalization_199[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_203 (Conv2D)             (None, 4, 4, 18)     14580       activation_202[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_196 (Dropout)           (None, 4, 4, 18)     0           conv2d_203[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_184 (Concatenate)   (None, 4, 4, 108)    0           concatenate_183[0][0]            \n",
            "                                                                 dropout_196[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_200 (BatchN (None, 4, 4, 108)    432         concatenate_184[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_203 (Activation)     (None, 4, 4, 108)    0           batch_normalization_200[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_204 (Conv2D)             (None, 4, 4, 18)     17496       activation_203[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_197 (Dropout)           (None, 4, 4, 18)     0           conv2d_204[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_185 (Concatenate)   (None, 4, 4, 126)    0           concatenate_184[0][0]            \n",
            "                                                                 dropout_197[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_201 (BatchN (None, 4, 4, 126)    504         concatenate_185[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_204 (Activation)     (None, 4, 4, 126)    0           batch_normalization_201[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_205 (Conv2D)             (None, 4, 4, 18)     20412       activation_204[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_198 (Dropout)           (None, 4, 4, 18)     0           conv2d_205[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_186 (Concatenate)   (None, 4, 4, 144)    0           concatenate_185[0][0]            \n",
            "                                                                 dropout_198[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_202 (BatchN (None, 4, 4, 144)    576         concatenate_186[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_205 (Activation)     (None, 4, 4, 144)    0           batch_normalization_202[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_206 (Conv2D)             (None, 4, 4, 18)     23328       activation_205[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_199 (Dropout)           (None, 4, 4, 18)     0           conv2d_206[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_187 (Concatenate)   (None, 4, 4, 162)    0           concatenate_186[0][0]            \n",
            "                                                                 dropout_199[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_203 (BatchN (None, 4, 4, 162)    648         concatenate_187[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_206 (Activation)     (None, 4, 4, 162)    0           batch_normalization_203[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_207 (Conv2D)             (None, 4, 4, 18)     26244       activation_206[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_200 (Dropout)           (None, 4, 4, 18)     0           conv2d_207[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_188 (Concatenate)   (None, 4, 4, 180)    0           concatenate_187[0][0]            \n",
            "                                                                 dropout_200[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_204 (BatchN (None, 4, 4, 180)    720         concatenate_188[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_207 (Activation)     (None, 4, 4, 180)    0           batch_normalization_204[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_208 (Conv2D)             (None, 4, 4, 18)     29160       activation_207[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_201 (Dropout)           (None, 4, 4, 18)     0           conv2d_208[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_189 (Concatenate)   (None, 4, 4, 198)    0           concatenate_188[0][0]            \n",
            "                                                                 dropout_201[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_205 (BatchN (None, 4, 4, 198)    792         concatenate_189[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_208 (Activation)     (None, 4, 4, 198)    0           batch_normalization_205[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_209 (Conv2D)             (None, 4, 4, 18)     32076       activation_208[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_202 (Dropout)           (None, 4, 4, 18)     0           conv2d_209[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_190 (Concatenate)   (None, 4, 4, 216)    0           concatenate_189[0][0]            \n",
            "                                                                 dropout_202[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_206 (BatchN (None, 4, 4, 216)    864         concatenate_190[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_209 (Activation)     (None, 4, 4, 216)    0           batch_normalization_206[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_210 (Conv2D)             (None, 4, 4, 18)     34992       activation_209[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_203 (Dropout)           (None, 4, 4, 18)     0           conv2d_210[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_191 (Concatenate)   (None, 4, 4, 234)    0           concatenate_190[0][0]            \n",
            "                                                                 dropout_203[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_207 (BatchN (None, 4, 4, 234)    936         concatenate_191[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_210 (Activation)     (None, 4, 4, 234)    0           batch_normalization_207[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_18 (AveragePo (None, 2, 2, 234)    0           activation_210[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_211 (Conv2D)             (None, 2, 2, 10)     21070       average_pooling2d_18[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_19 (AveragePo (None, 1, 1, 10)     0           conv2d_211[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "flatten_3 (Flatten)             (None, 10)           0           average_pooling2d_19[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "activation_211 (Activation)     (None, 10)           0           flatten_3[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 974,274\n",
            "Trainable params: 961,118\n",
            "Non-trainable params: 13,156\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9eMk5pT6yoG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from time import time\n",
        "from datetime import datetime\n",
        "from tensorflow.python.keras.callbacks import TensorBoard\n",
        "#https://keras.rstudio.com/reference/callback_model_checkpoint.html\n",
        "#https://machinelearningmastery.com/check-point-deep-learning-models-keras/\n",
        "filepath = \"weights.{epoch:02d}-{val_loss:.2f}.hdf5\"\n",
        "history = tf.keras.callbacks.History()\n",
        "\n",
        "# https://docs.w3cub.com/tensorflow~python/tf/keras/callbacks/reducelronplateau/\n",
        "tensorboard = TensorBoard(log_dir=\"model_logs/{}\".format(time()))\n",
        "\n",
        "filepath = \"weights.{epoch:02d}-{val_loss:.2f}.hdf5\"\n",
        "learning_rate_reduction = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_acc', \n",
        "                                            patience=3, \n",
        "                                            verbose=1, \n",
        "                                            factor=0.5, \n",
        "                                            min_lr=0.0001)\n",
        "checkpoint_save = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "\n",
        "callbacks_list = [checkpoint_save,learning_rate_reduction,history,tensorboard]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqMaXOZggCAc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#https://keras.io/preprocessing/image/\n",
        "#Data agumentation\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "datagen = ImageDataGenerator(\n",
        "    #featurewise_center=True, This was overfitting so commented this out\n",
        "    #featurewise_std_normalization=True,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest',\n",
        "    zoom_range=0.10)\n",
        "datagen.fit(X_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "b4XOsW3ahSkL",
        "colab": {}
      },
      "source": [
        "# determine Loss function and Optimizer\n",
        "# Tried with different optimizer Adam was giving best results\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2sRSnv4izCU",
        "colab_type": "code",
        "outputId": "9747c10e-7f38-40ce-b705-d588919014dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 10353
        }
      },
      "source": [
        "history = model.fit_generator(datagen.flow(X_train, y_train,\n",
        "                        batch_size=batch_size),callbacks= callbacks_list,\n",
        "                        epochs=epochs,verbose=0,\n",
        "                        validation_data=(X_test, y_test))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.40600, saving model to weights.01-2.26.hdf5\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.40600 to 0.41820, saving model to weights.02-2.86.hdf5\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.41820 to 0.51420, saving model to weights.03-2.06.hdf5\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.51420 to 0.67160, saving model to weights.04-1.04.hdf5\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.67160 to 0.70430, saving model to weights.05-0.94.hdf5\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.70430\n",
            "\n",
            "Epoch 00007: val_acc improved from 0.70430 to 0.71130, saving model to weights.07-0.93.hdf5\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.71130\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.71130\n",
            "\n",
            "Epoch 00010: val_acc improved from 0.71130 to 0.76760, saving model to weights.10-0.76.hdf5\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.76760\n",
            "\n",
            "Epoch 00012: val_acc improved from 0.76760 to 0.78900, saving model to weights.12-0.69.hdf5\n",
            "\n",
            "Epoch 00013: val_acc improved from 0.78900 to 0.79880, saving model to weights.13-0.67.hdf5\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.79880\n",
            "\n",
            "Epoch 00015: val_acc improved from 0.79880 to 0.80750, saving model to weights.15-0.61.hdf5\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.80750\n",
            "\n",
            "Epoch 00017: val_acc improved from 0.80750 to 0.82660, saving model to weights.17-0.55.hdf5\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.82660\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.82660\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.82660\n",
            "\n",
            "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\n",
            "Epoch 00021: val_acc improved from 0.82660 to 0.84220, saving model to weights.21-0.51.hdf5\n",
            "\n",
            "Epoch 00022: val_acc improved from 0.84220 to 0.85140, saving model to weights.22-0.48.hdf5\n",
            "\n",
            "Epoch 00023: val_acc improved from 0.85140 to 0.85190, saving model to weights.23-0.50.hdf5\n",
            "\n",
            "Epoch 00024: val_acc improved from 0.85190 to 0.86120, saving model to weights.24-0.48.hdf5\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.86120\n",
            "\n",
            "Epoch 00026: val_acc improved from 0.86120 to 0.86300, saving model to weights.26-0.45.hdf5\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.86300\n",
            "\n",
            "Epoch 00028: val_acc did not improve from 0.86300\n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.86300\n",
            "\n",
            "Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\n",
            "Epoch 00030: val_acc improved from 0.86300 to 0.87940, saving model to weights.30-0.39.hdf5\n",
            "\n",
            "Epoch 00031: val_acc improved from 0.87940 to 0.88260, saving model to weights.31-0.38.hdf5\n",
            "\n",
            "Epoch 00032: val_acc improved from 0.88260 to 0.88310, saving model to weights.32-0.38.hdf5\n",
            "\n",
            "Epoch 00033: val_acc did not improve from 0.88310\n",
            "\n",
            "Epoch 00034: val_acc did not improve from 0.88310\n",
            "\n",
            "Epoch 00035: val_acc did not improve from 0.88310\n",
            "\n",
            "Epoch 00035: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "\n",
            "Epoch 00036: val_acc improved from 0.88310 to 0.88540, saving model to weights.36-0.37.hdf5\n",
            "\n",
            "Epoch 00037: val_acc improved from 0.88540 to 0.88880, saving model to weights.37-0.37.hdf5\n",
            "\n",
            "Epoch 00038: val_acc did not improve from 0.88880\n",
            "\n",
            "Epoch 00039: val_acc improved from 0.88880 to 0.89180, saving model to weights.39-0.36.hdf5\n",
            "\n",
            "Epoch 00040: val_acc improved from 0.89180 to 0.89220, saving model to weights.40-0.35.hdf5\n",
            "\n",
            "Epoch 00041: val_acc improved from 0.89220 to 0.89570, saving model to weights.41-0.35.hdf5\n",
            "\n",
            "Epoch 00042: val_acc did not improve from 0.89570\n",
            "\n",
            "Epoch 00043: val_acc did not improve from 0.89570\n",
            "\n",
            "Epoch 00044: val_acc did not improve from 0.89570\n",
            "\n",
            "Epoch 00044: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
            "\n",
            "Epoch 00045: val_acc did not improve from 0.89570\n",
            "\n",
            "Epoch 00046: val_acc did not improve from 0.89570\n",
            "\n",
            "Epoch 00047: val_acc did not improve from 0.89570\n",
            "\n",
            "Epoch 00048: val_acc did not improve from 0.89570\n",
            "\n",
            "Epoch 00049: val_acc improved from 0.89570 to 0.89820, saving model to weights.49-0.34.hdf5\n",
            "\n",
            "Epoch 00050: val_acc did not improve from 0.89820\n",
            "\n",
            "Epoch 00051: val_acc did not improve from 0.89820\n",
            "\n",
            "Epoch 00052: val_acc did not improve from 0.89820\n",
            "\n",
            "Epoch 00053: val_acc did not improve from 0.89820\n",
            "\n",
            "Epoch 00054: val_acc did not improve from 0.89820\n",
            "\n",
            "Epoch 00055: val_acc improved from 0.89820 to 0.89850, saving model to weights.55-0.35.hdf5\n",
            "\n",
            "Epoch 00056: val_acc did not improve from 0.89850\n",
            "\n",
            "Epoch 00057: val_acc did not improve from 0.89850\n",
            "\n",
            "Epoch 00058: val_acc did not improve from 0.89850\n",
            "\n",
            "Epoch 00059: val_acc did not improve from 0.89850\n",
            "\n",
            "Epoch 00060: val_acc did not improve from 0.89850\n",
            "\n",
            "Epoch 00061: val_acc did not improve from 0.89850\n",
            "\n",
            "Epoch 00062: val_acc did not improve from 0.89850\n",
            "\n",
            "Epoch 00063: val_acc did not improve from 0.89850\n",
            "\n",
            "Epoch 00064: val_acc did not improve from 0.89850\n",
            "\n",
            "Epoch 00065: val_acc did not improve from 0.89850\n",
            "\n",
            "Epoch 00066: val_acc did not improve from 0.89850\n",
            "\n",
            "Epoch 00067: val_acc did not improve from 0.89850\n",
            "\n",
            "Epoch 00068: val_acc did not improve from 0.89850\n",
            "\n",
            "Epoch 00069: val_acc did not improve from 0.89850\n",
            "\n",
            "Epoch 00070: val_acc did not improve from 0.89850\n",
            "\n",
            "Epoch 00071: val_acc did not improve from 0.89850\n",
            "\n",
            "Epoch 00072: val_acc did not improve from 0.89850\n",
            "\n",
            "Epoch 00073: val_acc did not improve from 0.89850\n",
            "\n",
            "Epoch 00074: val_acc did not improve from 0.89850\n",
            "\n",
            "Epoch 00075: val_acc did not improve from 0.89850\n",
            "\n",
            "Epoch 00076: val_acc improved from 0.89850 to 0.89920, saving model to weights.76-0.34.hdf5\n",
            "\n",
            "Epoch 00077: val_acc did not improve from 0.89920\n",
            "\n",
            "Epoch 00078: val_acc did not improve from 0.89920\n",
            "\n",
            "Epoch 00079: val_acc did not improve from 0.89920\n",
            "\n",
            "Epoch 00080: val_acc did not improve from 0.89920\n",
            "\n",
            "Epoch 00081: val_acc improved from 0.89920 to 0.89960, saving model to weights.81-0.34.hdf5\n",
            "\n",
            "Epoch 00082: val_acc did not improve from 0.89960\n",
            "\n",
            "Epoch 00083: val_acc did not improve from 0.89960\n",
            "\n",
            "Epoch 00084: val_acc improved from 0.89960 to 0.90100, saving model to weights.84-0.33.hdf5\n",
            "\n",
            "Epoch 00085: val_acc did not improve from 0.90100\n",
            "\n",
            "Epoch 00086: val_acc did not improve from 0.90100\n",
            "\n",
            "Epoch 00087: val_acc did not improve from 0.90100\n",
            "\n",
            "Epoch 00088: val_acc did not improve from 0.90100\n",
            "\n",
            "Epoch 00089: val_acc improved from 0.90100 to 0.90170, saving model to weights.89-0.33.hdf5\n",
            "\n",
            "Epoch 00090: val_acc did not improve from 0.90170\n",
            "\n",
            "Epoch 00091: val_acc did not improve from 0.90170\n",
            "\n",
            "Epoch 00092: val_acc did not improve from 0.90170\n",
            "\n",
            "Epoch 00093: val_acc did not improve from 0.90170\n",
            "\n",
            "Epoch 00094: val_acc did not improve from 0.90170\n",
            "\n",
            "Epoch 00095: val_acc did not improve from 0.90170\n",
            "\n",
            "Epoch 00096: val_acc did not improve from 0.90170\n",
            "\n",
            "Epoch 00097: val_acc did not improve from 0.90170\n",
            "\n",
            "Epoch 00098: val_acc did not improve from 0.90170\n",
            "\n",
            "Epoch 00099: val_acc did not improve from 0.90170\n",
            "\n",
            "Epoch 00100: val_acc did not improve from 0.90170\n",
            "\n",
            "Epoch 00101: val_acc improved from 0.90170 to 0.90240, saving model to weights.101-0.32.hdf5\n",
            "\n",
            "Epoch 00102: val_acc did not improve from 0.90240\n",
            "\n",
            "Epoch 00103: val_acc improved from 0.90240 to 0.90260, saving model to weights.103-0.33.hdf5\n",
            "\n",
            "Epoch 00104: val_acc improved from 0.90260 to 0.90270, saving model to weights.104-0.34.hdf5\n",
            "\n",
            "Epoch 00105: val_acc did not improve from 0.90270\n",
            "\n",
            "Epoch 00106: val_acc did not improve from 0.90270\n",
            "\n",
            "Epoch 00107: val_acc did not improve from 0.90270\n",
            "\n",
            "Epoch 00108: val_acc did not improve from 0.90270\n",
            "\n",
            "Epoch 00109: val_acc improved from 0.90270 to 0.90510, saving model to weights.109-0.33.hdf5\n",
            "\n",
            "Epoch 00110: val_acc did not improve from 0.90510\n",
            "\n",
            "Epoch 00111: val_acc did not improve from 0.90510\n",
            "\n",
            "Epoch 00112: val_acc did not improve from 0.90510\n",
            "\n",
            "Epoch 00113: val_acc did not improve from 0.90510\n",
            "\n",
            "Epoch 00114: val_acc did not improve from 0.90510\n",
            "\n",
            "Epoch 00115: val_acc did not improve from 0.90510\n",
            "\n",
            "Epoch 00116: val_acc did not improve from 0.90510\n",
            "\n",
            "Epoch 00117: val_acc improved from 0.90510 to 0.90710, saving model to weights.117-0.32.hdf5\n",
            "\n",
            "Epoch 00118: val_acc did not improve from 0.90710\n",
            "\n",
            "Epoch 00119: val_acc did not improve from 0.90710\n",
            "\n",
            "Epoch 00120: val_acc did not improve from 0.90710\n",
            "\n",
            "Epoch 00121: val_acc did not improve from 0.90710\n",
            "\n",
            "Epoch 00122: val_acc did not improve from 0.90710\n",
            "\n",
            "Epoch 00123: val_acc did not improve from 0.90710\n",
            "\n",
            "Epoch 00124: val_acc did not improve from 0.90710\n",
            "\n",
            "Epoch 00125: val_acc did not improve from 0.90710\n",
            "\n",
            "Epoch 00126: val_acc did not improve from 0.90710\n",
            "\n",
            "Epoch 00127: val_acc did not improve from 0.90710\n",
            "\n",
            "Epoch 00128: val_acc did not improve from 0.90710\n",
            "\n",
            "Epoch 00129: val_acc did not improve from 0.90710\n",
            "\n",
            "Epoch 00130: val_acc did not improve from 0.90710\n",
            "\n",
            "Epoch 00131: val_acc did not improve from 0.90710\n",
            "\n",
            "Epoch 00132: val_acc did not improve from 0.90710\n",
            "\n",
            "Epoch 00133: val_acc did not improve from 0.90710\n",
            "\n",
            "Epoch 00134: val_acc did not improve from 0.90710\n",
            "\n",
            "Epoch 00135: val_acc did not improve from 0.90710\n",
            "\n",
            "Epoch 00136: val_acc did not improve from 0.90710\n",
            "\n",
            "Epoch 00137: val_acc did not improve from 0.90710\n",
            "\n",
            "Epoch 00138: val_acc improved from 0.90710 to 0.90720, saving model to weights.138-0.32.hdf5\n",
            "\n",
            "Epoch 00139: val_acc did not improve from 0.90720\n",
            "\n",
            "Epoch 00140: val_acc did not improve from 0.90720\n",
            "\n",
            "Epoch 00141: val_acc did not improve from 0.90720\n",
            "\n",
            "Epoch 00142: val_acc did not improve from 0.90720\n",
            "\n",
            "Epoch 00143: val_acc did not improve from 0.90720\n",
            "\n",
            "Epoch 00144: val_acc did not improve from 0.90720\n",
            "\n",
            "Epoch 00145: val_acc did not improve from 0.90720\n",
            "\n",
            "Epoch 00146: val_acc did not improve from 0.90720\n",
            "\n",
            "Epoch 00147: val_acc did not improve from 0.90720\n",
            "\n",
            "Epoch 00148: val_acc did not improve from 0.90720\n",
            "\n",
            "Epoch 00149: val_acc did not improve from 0.90720\n",
            "\n",
            "Epoch 00150: val_acc did not improve from 0.90720\n",
            "\n",
            "Epoch 00151: val_acc did not improve from 0.90720\n",
            "\n",
            "Epoch 00152: val_acc did not improve from 0.90720\n",
            "\n",
            "Epoch 00153: val_acc did not improve from 0.90720\n",
            "\n",
            "Epoch 00154: val_acc did not improve from 0.90720\n",
            "\n",
            "Epoch 00155: val_acc did not improve from 0.90720\n",
            "\n",
            "Epoch 00156: val_acc did not improve from 0.90720\n",
            "\n",
            "Epoch 00157: val_acc did not improve from 0.90720\n",
            "\n",
            "Epoch 00158: val_acc did not improve from 0.90720\n",
            "\n",
            "Epoch 00159: val_acc did not improve from 0.90720\n",
            "\n",
            "Epoch 00160: val_acc improved from 0.90720 to 0.90800, saving model to weights.160-0.33.hdf5\n",
            "\n",
            "Epoch 00161: val_acc did not improve from 0.90800\n",
            "\n",
            "Epoch 00162: val_acc did not improve from 0.90800\n",
            "\n",
            "Epoch 00163: val_acc did not improve from 0.90800\n",
            "\n",
            "Epoch 00164: val_acc did not improve from 0.90800\n",
            "\n",
            "Epoch 00165: val_acc did not improve from 0.90800\n",
            "\n",
            "Epoch 00166: val_acc did not improve from 0.90800\n",
            "\n",
            "Epoch 00167: val_acc did not improve from 0.90800\n",
            "\n",
            "Epoch 00168: val_acc did not improve from 0.90800\n",
            "\n",
            "Epoch 00169: val_acc did not improve from 0.90800\n",
            "\n",
            "Epoch 00170: val_acc did not improve from 0.90800\n",
            "\n",
            "Epoch 00171: val_acc did not improve from 0.90800\n",
            "\n",
            "Epoch 00172: val_acc did not improve from 0.90800\n",
            "\n",
            "Epoch 00173: val_acc did not improve from 0.90800\n",
            "\n",
            "Epoch 00174: val_acc did not improve from 0.90800\n",
            "\n",
            "Epoch 00175: val_acc did not improve from 0.90800\n",
            "\n",
            "Epoch 00176: val_acc improved from 0.90800 to 0.90820, saving model to weights.176-0.33.hdf5\n",
            "\n",
            "Epoch 00177: val_acc did not improve from 0.90820\n",
            "\n",
            "Epoch 00178: val_acc improved from 0.90820 to 0.90870, saving model to weights.178-0.33.hdf5\n",
            "\n",
            "Epoch 00179: val_acc did not improve from 0.90870\n",
            "\n",
            "Epoch 00180: val_acc did not improve from 0.90870\n",
            "\n",
            "Epoch 00181: val_acc did not improve from 0.90870\n",
            "\n",
            "Epoch 00182: val_acc did not improve from 0.90870\n",
            "\n",
            "Epoch 00183: val_acc did not improve from 0.90870\n",
            "\n",
            "Epoch 00184: val_acc did not improve from 0.90870\n",
            "\n",
            "Epoch 00185: val_acc did not improve from 0.90870\n",
            "\n",
            "Epoch 00186: val_acc did not improve from 0.90870\n",
            "\n",
            "Epoch 00187: val_acc did not improve from 0.90870\n",
            "\n",
            "Epoch 00188: val_acc did not improve from 0.90870\n",
            "\n",
            "Epoch 00189: val_acc did not improve from 0.90870\n",
            "\n",
            "Epoch 00190: val_acc did not improve from 0.90870\n",
            "\n",
            "Epoch 00191: val_acc did not improve from 0.90870\n",
            "\n",
            "Epoch 00192: val_acc did not improve from 0.90870\n",
            "\n",
            "Epoch 00193: val_acc did not improve from 0.90870\n",
            "\n",
            "Epoch 00194: val_acc improved from 0.90870 to 0.91050, saving model to weights.194-0.32.hdf5\n",
            "\n",
            "Epoch 00195: val_acc improved from 0.91050 to 0.91100, saving model to weights.195-0.32.hdf5\n",
            "\n",
            "Epoch 00196: val_acc did not improve from 0.91100\n",
            "\n",
            "Epoch 00197: val_acc did not improve from 0.91100\n",
            "\n",
            "Epoch 00198: val_acc did not improve from 0.91100\n",
            "\n",
            "Epoch 00199: val_acc did not improve from 0.91100\n",
            "\n",
            "Epoch 00200: val_acc did not improve from 0.91100\n",
            "\n",
            "Epoch 00201: val_acc did not improve from 0.91100\n",
            "\n",
            "Epoch 00202: val_acc did not improve from 0.91100\n",
            "\n",
            "Epoch 00203: val_acc did not improve from 0.91100\n",
            "\n",
            "Epoch 00204: val_acc did not improve from 0.91100\n",
            "\n",
            "Epoch 00205: val_acc did not improve from 0.91100\n",
            "\n",
            "Epoch 00206: val_acc improved from 0.91100 to 0.91120, saving model to weights.206-0.32.hdf5\n",
            "\n",
            "Epoch 00207: val_acc did not improve from 0.91120\n",
            "\n",
            "Epoch 00208: val_acc improved from 0.91120 to 0.91180, saving model to weights.208-0.32.hdf5\n",
            "\n",
            "Epoch 00209: val_acc did not improve from 0.91180\n",
            "\n",
            "Epoch 00210: val_acc did not improve from 0.91180\n",
            "\n",
            "Epoch 00211: val_acc did not improve from 0.91180\n",
            "\n",
            "Epoch 00212: val_acc did not improve from 0.91180\n",
            "\n",
            "Epoch 00213: val_acc did not improve from 0.91180\n",
            "\n",
            "Epoch 00214: val_acc did not improve from 0.91180\n",
            "\n",
            "Epoch 00215: val_acc did not improve from 0.91180\n",
            "\n",
            "Epoch 00216: val_acc did not improve from 0.91180\n",
            "\n",
            "Epoch 00217: val_acc did not improve from 0.91180\n",
            "\n",
            "Epoch 00218: val_acc did not improve from 0.91180\n",
            "\n",
            "Epoch 00219: val_acc did not improve from 0.91180\n",
            "\n",
            "Epoch 00220: val_acc improved from 0.91180 to 0.91290, saving model to weights.220-0.32.hdf5\n",
            "\n",
            "Epoch 00221: val_acc did not improve from 0.91290\n",
            "\n",
            "Epoch 00222: val_acc did not improve from 0.91290\n",
            "\n",
            "Epoch 00223: val_acc did not improve from 0.91290\n",
            "\n",
            "Epoch 00224: val_acc did not improve from 0.91290\n",
            "\n",
            "Epoch 00225: val_acc did not improve from 0.91290\n",
            "\n",
            "Epoch 00226: val_acc did not improve from 0.91290\n",
            "\n",
            "Epoch 00227: val_acc did not improve from 0.91290\n",
            "\n",
            "Epoch 00228: val_acc did not improve from 0.91290\n",
            "\n",
            "Epoch 00229: val_acc did not improve from 0.91290\n",
            "\n",
            "Epoch 00230: val_acc did not improve from 0.91290\n",
            "\n",
            "Epoch 00231: val_acc did not improve from 0.91290\n",
            "\n",
            "Epoch 00232: val_acc did not improve from 0.91290\n",
            "\n",
            "Epoch 00233: val_acc did not improve from 0.91290\n",
            "\n",
            "Epoch 00234: val_acc did not improve from 0.91290\n",
            "\n",
            "Epoch 00235: val_acc did not improve from 0.91290\n",
            "\n",
            "Epoch 00236: val_acc did not improve from 0.91290\n",
            "\n",
            "Epoch 00237: val_acc improved from 0.91290 to 0.91410, saving model to weights.237-0.32.hdf5\n",
            "\n",
            "Epoch 00238: val_acc did not improve from 0.91410\n",
            "\n",
            "Epoch 00239: val_acc did not improve from 0.91410\n",
            "\n",
            "Epoch 00240: val_acc did not improve from 0.91410\n",
            "\n",
            "Epoch 00241: val_acc did not improve from 0.91410\n",
            "\n",
            "Epoch 00242: val_acc did not improve from 0.91410\n",
            "\n",
            "Epoch 00243: val_acc did not improve from 0.91410\n",
            "\n",
            "Epoch 00244: val_acc did not improve from 0.91410\n",
            "\n",
            "Epoch 00245: val_acc did not improve from 0.91410\n",
            "\n",
            "Epoch 00246: val_acc did not improve from 0.91410\n",
            "\n",
            "Epoch 00247: val_acc did not improve from 0.91410\n",
            "\n",
            "Epoch 00248: val_acc did not improve from 0.91410\n",
            "\n",
            "Epoch 00249: val_acc did not improve from 0.91410\n",
            "\n",
            "Epoch 00250: val_acc did not improve from 0.91410\n",
            "\n",
            "Epoch 00251: val_acc did not improve from 0.91410\n",
            "\n",
            "Epoch 00252: val_acc did not improve from 0.91410\n",
            "\n",
            "Epoch 00253: val_acc did not improve from 0.91410\n",
            "\n",
            "Epoch 00254: val_acc did not improve from 0.91410\n",
            "\n",
            "Epoch 00255: val_acc did not improve from 0.91410\n",
            "\n",
            "Epoch 00256: val_acc did not improve from 0.91410\n",
            "\n",
            "Epoch 00257: val_acc did not improve from 0.91410\n",
            "\n",
            "Epoch 00258: val_acc did not improve from 0.91410\n",
            "\n",
            "Epoch 00259: val_acc did not improve from 0.91410\n",
            "\n",
            "Epoch 00260: val_acc did not improve from 0.91410\n",
            "\n",
            "Epoch 00261: val_acc did not improve from 0.91410\n",
            "\n",
            "Epoch 00262: val_acc did not improve from 0.91410\n",
            "\n",
            "Epoch 00263: val_acc did not improve from 0.91410\n",
            "\n",
            "Epoch 00264: val_acc did not improve from 0.91410\n",
            "\n",
            "Epoch 00265: val_acc did not improve from 0.91410\n",
            "\n",
            "Epoch 00266: val_acc did not improve from 0.91410\n",
            "\n",
            "Epoch 00267: val_acc did not improve from 0.91410\n",
            "\n",
            "Epoch 00268: val_acc did not improve from 0.91410\n",
            "\n",
            "Epoch 00269: val_acc did not improve from 0.91410\n",
            "\n",
            "Epoch 00270: val_acc did not improve from 0.91410\n",
            "\n",
            "Epoch 00271: val_acc did not improve from 0.91410\n",
            "\n",
            "Epoch 00272: val_acc did not improve from 0.91410\n",
            "\n",
            "Epoch 00273: val_acc did not improve from 0.91410\n",
            "\n",
            "Epoch 00274: val_acc did not improve from 0.91410\n",
            "\n",
            "Epoch 00275: val_acc did not improve from 0.91410\n",
            "\n",
            "Epoch 00276: val_acc did not improve from 0.91410\n",
            "\n",
            "Epoch 00277: val_acc did not improve from 0.91410\n",
            "\n",
            "Epoch 00278: val_acc did not improve from 0.91410\n",
            "\n",
            "Epoch 00279: val_acc did not improve from 0.91410\n",
            "\n",
            "Epoch 00280: val_acc did not improve from 0.91410\n",
            "\n",
            "Epoch 00281: val_acc did not improve from 0.91410\n",
            "\n",
            "Epoch 00282: val_acc did not improve from 0.91410\n",
            "\n",
            "Epoch 00283: val_acc did not improve from 0.91410\n",
            "\n",
            "Epoch 00284: val_acc did not improve from 0.91410\n",
            "\n",
            "Epoch 00285: val_acc did not improve from 0.91410\n",
            "\n",
            "Epoch 00286: val_acc did not improve from 0.91410\n",
            "\n",
            "Epoch 00287: val_acc did not improve from 0.91410\n",
            "\n",
            "Epoch 00288: val_acc did not improve from 0.91410\n",
            "\n",
            "Epoch 00289: val_acc did not improve from 0.91410\n",
            "\n",
            "Epoch 00290: val_acc did not improve from 0.91410\n",
            "\n",
            "Epoch 00291: val_acc did not improve from 0.91410\n",
            "\n",
            "Epoch 00292: val_acc improved from 0.91410 to 0.91480, saving model to weights.292-0.32.hdf5\n",
            "\n",
            "Epoch 00293: val_acc did not improve from 0.91480\n",
            "\n",
            "Epoch 00294: val_acc improved from 0.91480 to 0.91810, saving model to weights.294-0.31.hdf5\n",
            "\n",
            "Epoch 00295: val_acc did not improve from 0.91810\n",
            "\n",
            "Epoch 00296: val_acc did not improve from 0.91810\n",
            "\n",
            "Epoch 00297: val_acc did not improve from 0.91810\n",
            "\n",
            "Epoch 00298: val_acc did not improve from 0.91810\n",
            "\n",
            "Epoch 00299: val_acc did not improve from 0.91810\n",
            "\n",
            "Epoch 00300: val_acc did not improve from 0.91810\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UE3lF6EH1r_L",
        "colab": {}
      },
      "source": [
        "# Save the trained weights in to .h5 format weights.294-0.31.hdf5\n",
        "model.load_weights(\"/content/weights.294-0.31.hdf5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yliVnyDAaEzf",
        "colab_type": "code",
        "outputId": "36fffee7-2658-430b-9137-227d56e0f412",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Test the model\n",
        "score = model.evaluate(X_test, y_test, verbose=1)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 5s 461us/sample - loss: 0.3064 - acc: 0.9181\n",
            "Test loss: 0.3063799827516079\n",
            "Test accuracy: 0.9181\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48Vf_k4VkyDD",
        "colab_type": "code",
        "outputId": "d6240e8d-0854-4c20-80d7-98436d863fa7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "plt.plot(history.history['acc'], 'r')\n",
        "plt.plot(history.history['val_acc'], 'b')\n",
        "plt.legend({'Train Accuracy': 'r', 'Test Accuracy':'b'})\n",
        "plt.xlabel('Epoch')\n",
        "plt.show()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEMCAYAAADOLq1xAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ng6pMgxn/hwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}